{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMjP5diW9B/aqPquAeD7w6b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#IMPORTANT NOTES:-\n","##<>How Random Forest Assigns Feature Importance\n","Random Forest determines feature importance based on how much each feature reduces impurity in decision trees. Let's break it down step by step:\n","\n","1. Impurity Reduction (Gini/Entropy)\n","Each decision tree in the Random Forest makes splits on different features. At each split:\n","\n","The algorithm selects a feature and a threshold to split the data.\n","\n","It calculates how much that split reduces impurity (Gini impurity or Entropy).\n","\n","The reduction in impurity is attributed to that feature.\n","\n","Example:\n","If splitting on \"age\" significantly reduces impurity, \"age\" gets a higher importance score.\n","\n","2. Accumulating Importance Across Trees\n","Since a Random Forest is made up of multiple decision trees, it calculates feature importance for each tree individually and then averages them across all trees.\n","\n","For each feature:\n","\n","Feature Importance\n","=\n","∑\n","(\n","Impurity Reduction from splits on the feature\n",")\n","/\n","Total Trees\n","Feature Importance=∑(Impurity Reduction from splits on the feature)/Total Trees\n","Thus, if a feature is frequently chosen and contributes to large impurity reductions, it will have a higher importance score.\n","\n","3. Normalization of Importance Scores\n","Once the importance scores are computed for all features, they are normalized so that they sum to 1.\n","This makes it easier to compare relative importance.\n","\n","Types of Feature Importance in RandomForestClassifier\n","Mean Decrease in Impurity (MDI) – Used Here\n","\n","Directly based on impurity reduction in decision trees.\n","\n","This is what rfcf.feature_importances_ provides.\n","\n","Mean Decrease in Accuracy (MDA) – Permutation Importance\n","\n","Measures how much accuracy drops when a feature's values are randomly shuffled.\n","\n","Requires separate calculation (not provided by .feature_importances_).\n","\n","\n","\n","#Hyperparameter Tuning & Code Explanation\n","What is Hyperparameter Tuning?\n","Hyperparameter tuning is the process of selecting the best hyperparameters for a machine learning model to maximize its performance.\n","\n","Hyperparameters are model settings not learned from data (e.g., number of trees in a Random Forest, depth of trees, etc.).\n","\n","The goal is to find the optimal combination of hyperparameters that gives the best accuracy or minimizes error.\n","\n","Techniques like GridSearchCV and RandomizedSearchCV automate the search for optimal parameters.\n","\n","\n","#Why Use Hyperparameter Tuning?\n","Improves Model Performance 🚀\n","\n","Avoids underfitting or overfitting by finding the best settings.\n","\n","Automates Search for Best Parameters 🔍\n","\n","Instead of manually trying different values, GridSearchCV tests all combinations.\n","\n","Ensures Robustness 📊\n","\n","Uses cross-validation to test performance on multiple subsets of the data.\n","\n","\n"],"metadata":{"id":"I2A6IYoMeNgJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"OeqJ2bgueMqc"},"outputs":[],"source":[]}]}